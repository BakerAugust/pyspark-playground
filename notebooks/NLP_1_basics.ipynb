{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d2f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.0-bin-hadoop3.2')\n",
    "DATA_PATH = '../data/Spark_for_Machine_Learning/Natural_Language_Processing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694be604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/30 14:12:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf2abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479f74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = spark.createDataFrame([\n",
    "    (0, 'I like burritos.'),\n",
    "    (1, 'She prefers tacos.'),\n",
    "    (2, 'We can go and get both at the restaurant.'),\n",
    "    (3, 'They,serve,pozole,tacos,burritos,and,ceviche.'),\n",
    "    (4, 'Mezcal and Tequila, too.')\n",
    "], ['id','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d3ebfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|    I like burritos.|\n",
      "|  1|  She prefers tacos.|\n",
      "|  2|We can go and get...|\n",
      "|  3|They,serve,pozole...|\n",
      "|  4|Mezcal and Tequil...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentences.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89aaf2",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a208e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "regex_tokenizer = RegexTokenizer(\n",
    "    inputCol='sentence', outputCol='words', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf6c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda words:len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc12ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|    I like burritos.|[i, like, burritos.]|     3|\n",
      "|  1|  She prefers tacos.|[she, prefers, ta...|     3|\n",
      "|  2|We can go and get...|[we, can, go, and...|     9|\n",
      "|  3|They,serve,pozole...|[they,serve,pozol...|     1|\n",
      "|  4|Mezcal and Tequil...|[mezcal, and, teq...|     4|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic tokenizer requires whitespace\n",
    "tokenized = tokenizer.transform(sentences)\n",
    "tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d27b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our regex tokenizer will split the sentences appropriately.\n",
    "rg_tokenized = regex_tokenizer.transform(sentences)\n",
    "rg_tokenized = rg_tokenized.withColumn('tokens', count_tokens(col('words')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc95ecc",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f755d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85e92953",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f2ecc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            filtered|\n",
      "+--------------------+\n",
      "|    [like, burritos]|\n",
      "|    [prefers, tacos]|\n",
      "|[go, get, restaur...|\n",
      "|[serve, pozole, t...|\n",
      "|   [mezcal, tequila]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(rg_tokenized).select('filtered').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a6db6",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b528ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cea07796",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol='words',outputCol='grams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b8520a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|grams                                                                              |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|[i like, like burritos]                                                            |\n",
      "|[she prefers, prefers tacos]                                                       |\n",
      "|[we can, can go, go and, and get, get both, both at, at the, the restaurant]       |\n",
      "|[they serve, serve pozole, pozole tacos, tacos burritos, burritos and, and ceviche]|\n",
      "|[mezcal and, and tequila, tequila too]                                             |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram.transform(rg_tokenized).select('grams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac96d2",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Term-frequency inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99938ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b579c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "|sentence                                     |rawFeatures                                                                                                  |\n",
      "+---------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "|I like burritos.                             |(262144,[19036,28128,208258],[1.0,1.0,1.0])                                                                  |\n",
      "|She prefers tacos.                           |(262144,[54216,89302,211980],[1.0,1.0,1.0])                                                                  |\n",
      "|We can go and get both at the restaurant.    |(262144,[30497,95889,108437,148675,156084,162111,187114,219915,252722],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|They,serve,pozole,tacos,burritos,and,ceviche.|(262144,[11949,28128,89302,151536,203770,219915,223020],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |\n",
      "|Mezcal and Tequila, too.                     |(262144,[5923,49396,155063,219915],[1.0,1.0,1.0,1.0])                                                        |\n",
      "+---------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get TF features\n",
    "hashing_tf = HashingTF(inputCol='words',outputCol='rawFeatures')\n",
    "featurized_data = hashing_tf.transform(rg_tokenized)\n",
    "featurized_data.select(['sentence','rawFeatures']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ac59fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IDF\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idf_fit = idf.fit(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbdfbbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[19036,28128,208258],[1.0986122886681098,0.6931471805599453,1.0986122886681098])                                                                                                                                                            |\n",
      "|(262144,[54216,89302,211980],[1.0986122886681098,0.6931471805599453,1.0986122886681098])                                                                                                                                                            |\n",
      "|(262144,[30497,95889,108437,148675,156084,162111,187114,219915,252722],[1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.4054651081081644,1.0986122886681098])|\n",
      "|(262144,[11949,28128,89302,151536,203770,219915,223020],[1.0986122886681098,0.6931471805599453,0.6931471805599453,1.0986122886681098,1.0986122886681098,0.4054651081081644,1.0986122886681098])                                                     |\n",
      "|(262144,[5923,49396,155063,219915],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.4054651081081644])                                                                                                                                   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/30 14:32:12 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    }
   ],
   "source": [
    "idf_fit.transform(featurized_data).select('features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc8dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
