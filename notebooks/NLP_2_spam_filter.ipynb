{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d2f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.0-bin-hadoop3.2')\n",
    "DATA_PATH = '../data/Spark_for_Machine_Learning/Natural_Language_Processing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694be604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/30 20:46:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4399f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(DATA_PATH + 'smsspamcollection/SMSSpamCollection', \n",
    "               inferSchema=True,\n",
    "               sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d000cdb",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67acc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data\n",
    "        .withColumnRenamed('_c0', 'class')\n",
    "        .withColumnRenamed('_c1', 'sentence')\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4a1a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|class|\n",
      "+-----+\n",
      "|  ham|\n",
      "| spam|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615ec079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, col\n",
    "from pyspark.ml.feature import (Tokenizer, StopWordsRemover, \n",
    "                                CountVectorizer, IDF, \n",
    "                                StringIndexer, VectorAssembler)\n",
    "from pyspark.ml.classification import RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a99ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length', length(col('sentence')))\n",
    "tokenizer = Tokenizer(inputCol='sentence',outputCol='token_text')\n",
    "stop_remove = StopWordsRemover(inputCol='token_text', outputCol='tokens')\n",
    "count_vec = CountVectorizer(inputCol='tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec',outputCol='tf_idf')\n",
    "ham_spam_to_numeric = StringIndexer(inputCol='class', outputCol='label')\n",
    "vec_assembler = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a3545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    stop_remove, \n",
    "    count_vec, \n",
    "    idf, \n",
    "    ham_spam_to_numeric, \n",
    "    vec_assembler,\n",
    "    svm,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02884135",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "451bee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify and split\n",
    "train0, test0 = data.filter(col('class')=='ham').randomSplit([0.7,0.3])\n",
    "train1, test1 = data.filter(col('class')=='spam').randomSplit([0.7,0.3])\n",
    "\n",
    "train = train0.union(train1) \n",
    "test = test0.union(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "144820df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/30 20:48:35 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "21/12/30 20:48:35 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "21/12/30 20:48:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/12/30 20:48:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "fit_model = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7db5c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = fit_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86774626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8cd126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f09d99bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3f797f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/30 20:49:45 WARN DAGScheduler: Broadcasting large task binary with size 1063.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.982029673473938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
