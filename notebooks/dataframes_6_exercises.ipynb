{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a64be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.0-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ed9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45bb7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out some date functions\n",
    "from pyspark.sql.functions import format_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cc67dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/23 17:00:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('exercise').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a3844",
   "metadata": {},
   "source": [
    "## Read-in data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8e093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../data/Spark_DataFrame_Project_Exercise/walmart_stock.csv', \n",
    "                     inferSchema=True, \n",
    "                     header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50b641c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fc26ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2012-01-03', Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
       " Row(Date='2012-01-04', Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
       " Row(Date='2012-01-05', Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
       " Row(Date='2012-01-06', Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922),\n",
       " Row(Date='2012-01-09', Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e85b11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+----+----+-----+--------+---------+\n",
      "|summary|      date|Open|High| Low|Close|  Volume|Adj Close|\n",
      "+-------+----------+----+----+----+-----+--------+---------+\n",
      "|  count|      1258|1258|1258|1258| 1258|    1258|     1258|\n",
      "|   mean|      null|  72|  72|  71|   72| 8222093|       67|\n",
      "| stddev|      null|   6|   6|   6|    6| 4519780|        6|\n",
      "|    min|2012-01-03|  56|  57|  56|   56| 2094900|       50|\n",
      "|    max|2016-12-30|  90|  90|  89|   90|80898100|       84|\n",
      "+-------+----------+----+----+----+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "desc_df = df.describe()\n",
    "desc_df.select([desc_df['summary'], desc_df['date']]+[desc_df[i].cast('integer') for i in desc_df.columns[2:]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52301c09",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b1a879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High Price vs. volume (HV) ratio\n",
    "hv_ratio = df.select(\n",
    "    (df['High'] / df['Volume']).alias('HV Ratio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03c37f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hv_ratio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69572d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2015-01-13')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Day of highest price?\n",
    "max_price = df.agg({'High':'max'}).collect()[0][0]\n",
    "df.filter(df['High']==max_price).select('Date').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67d8db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean close?\n",
    "df.agg({'Close': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8206669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|    2094900|   80898100|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min / max volume?\n",
    "from pyspark.sql.functions import min, max\n",
    "df.agg(min('Volume'), max('Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "149c7ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Days with close less than 60?\n",
    "df.filter('Close < 60').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b93baa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09141494435612083"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percent of time with High > 80?\n",
    "\n",
    "df.filter('High > 80').count() / df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c0ee406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| corr(High, Volume)|\n",
      "+-------------------+\n",
      "|-0.3384326061737161|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pearson's corr between volume and high?\n",
    "from pyspark.sql.functions import corr\n",
    "df.select(corr(df['High'], df['Volume'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c4ff31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max high per year?\n",
    "from pyspark.sql.functions import month\n",
    "(df.withColumn(\"Month\", month(\"Date\"))\n",
    " .groupby('Month')\n",
    " .mean()\n",
    " .select('Month','avg(Close)')\n",
    " .orderBy('Month')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04740997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
